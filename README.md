# Тестирование связки SAM3 + YOLO в задачах сегментации объектов

## Обзор проекта
Проект реализует интеграцию новых моделей **YOLO12s** (Ultralytics) для детекции объектов с моделью **SAM3** (Segment Anything 3) для точной instance-сегментации. Решение обеспечивает автоматическую разметку данных в задачах компьютерного зрения.

## Практическая значимость решения
*Данный подход решает проблему дефицита размеченных данных для задач сегментации, используя для этого детекцию объектов — для которой данные найти или разметить значительно проще.*
*Использование сегментации критически важно, когда требуется точная форма объекта, а не только его центральная точка (как в случае детекции).*

**Пример**: *на мусоросортировочном конвейере помимо типа отхода важна геометрия объекта для выбора оптимальной точки удара прессом, захвата манипулятором или направления воздушного потока.*

**Почему YOLO + SAM3 лучше чистого SAM3**:
- YOLO проще и быстрее обучиться детекции благодаря обилию доступных датасетов
- Сама разметка для детекции значительно проще разметки для сегментации


## Тестируемые датасеты

* **[WaRP - Waste Recycling Plant Dataset](https://www.kaggle.com/datasets/parohod/warp-waste-recycling-plant-dataset)** - Основной датасет, позволяющий оценить эффективность решения

  *Особенности:*
  - Среднее количество изображений на класс: **25–234**
  - Размеченные данные для сегментации: **отдельные кропы**
  - Расположение объектов: **близкое расположение** друг к другу при **высоком уровне сходства**

* **[Animal Image Dataset - Cats, Dogs, and Foxes](https://www.kaggle.com/datasets/snmahsa/animal-image-dataset-cats-dogs-and-foxes)** - Дополнительный датасет для оценки эффективности решения на относительно простых задачах

  *Особенности:*
  - Среднее количество изображений на класс: **103**
  - Размеченные данные для сегментации: **отсутствуют**
  - Расположение объектов: **большинство расположено изолированно** друг от друга без перекрытий


## Принцип работы решения

Архитектура проекта реализует последовательный двухэтапный пайплайн, объединяющий скоростную детекцию и прецизионную сегментацию

### Этап 1: Детекция объектов (YOLO12s)
Для каждого целевого датасета идет предварительное обучение, после чео модель:
* **Обнаруживает** все объекты целевых классов на изображении.
* **Возвращает** для каждого объекта:
    * Координаты ограничивающего прямоугольника (**Bounding Box**)
    * Имя/класс объекта (**Label**)

### Этап 2: Точная сегментация (SAM3)
Затем полученные данные передаются в модель **SAM3**, которая выполняет уточняющую сегментацию. SAM3 поддерживает несколько режимов работы:

| Режим ввода | Описание | Применение |
|-------------|----------|------------|
| **По Bounding Box** | SAM3 получает координаты bbox и выполняет сегментацию строго в его границах. | Когда важна точная пространственная локализация. |
| **По текстовому описанию** | Передаются имена классов (например, "bottle-oil", "cat"), SAM3 ищет соответствующие объекты на всём изображении. | Для open-vocabulary сегментации, когда bbox недоступны. |
| **Комбинированный** | Одновременно передаются и bbox, и текстовые промпты. SAM3 использует оба сигнала для максимальной точности. | Основной режим работы пайплайна — использует сильные стороны обоих подходов. |


## Итоговые результаты
* **Высокие метрики:** Достигнут cgF1 до **37.2** по бенчмаркам Sa-Co
* **Zero-shot & Open-Vocabulary:** Поддержка детекции и сегментации для классов, не встречавшихся при обучении
* **Ключевые метрики YOLO12s:**
    * `mAP50` = **0.489**
    * `mAP50-95` = **0.381**
 
Связка моделей YoLo12s и SAM3 показала эффективность в решении задачи сегментации при недостаточном количестве размеченных данных. Связка позволила упростить задачу, сведя ее к детекции.

Сегментация концепций с подсказками в SAM3 с одной стороны позволяет находить большее количество масок, а с другой стороны приводит к искажениям (сегментации иных классов).

Дальнейшие планы включают в себя сбор полученных масок и их последующую корректировку для обучения более быстрой YoLo11s-seg.



Ключевые показатели:
* По бенчмаркам Sa-Co результат cgF1 = 37.2
* Модель YOLO12s показала mAP50 = 0.489 и mAP50-95 = 0.381
* Решение поддерживает zero-shot режим — может находить и сегментировать объекты, которые не встречались в обучающей выборке

Главный вывод: связка YOLO12s и SAM3 хорошо показала себя в условиях нехватки размеченных данных. Вместо того чтобы учить модель сегментации с нуля, задача свелась к более простой — детекции, а SAM3 уже провела сегоментацию. Подход с концептуальными подсказками в SAM3 помогает находить больше объектов, но иногда модель сегментирует лишнее (объекты других классов).

В планах — собрать все полученные маски, вручную или автоматически их скорректировать и использовать для обучения более легкой и быстрой версии — YOLO11s-seg.

