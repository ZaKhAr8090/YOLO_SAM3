# Тестирование связки SAM3 + YOLO в задачах сегментации объектов

## Обзор проекта
Проект реализует интеграцию новых моделей **YOLO12s** (Ultralytics) для детекции объектов с моделью **SAM3** (Segment Anything 3) для точной instance-сегментации. Решение обеспечивает автоматическую разметку данных в задачах компьютерного зрения.

## Практическая значимость решения
*Данный подход решает проблему дефицита размеченных данных для задач сегментации, используя для этого детекцию объектов — для которой данные найти или разметить значительно проще.*
*Использование сегментации критически важно, когда требуется точная форма объекта, а не только его центральная точка (как в случае детекции).*

**Пример**: *на мусоросортировочном конвейере помимо типа отхода важна геометрия объекта для выбора оптимальной точки удара прессом, захвата манипулятором или направления воздушного потока.*

**Почему YOLO + SAM3 лучше чистого SAM3**:
- YOLO проще и быстрее обучиться детекции благодаря обилию доступных датасетов
- Сама разметка для детекции значительно проще разметки для сегментации


## Тестируемые датасеты

* **[WaRP - Waste Recycling Plant Dataset](https://www.kaggle.com/datasets/parohod/warp-waste-recycling-plant-dataset)** - Основной датасет, позволяющий оценить эффективность решения

  *Особенности:*
  - Среднее количество изображений на класс: **25–234**
  - Размеченные данные для сегментации: **отдельные кропы**
  - Расположение объектов: **близкое расположение** друг к другу при **высоком уровне сходства**

* **[Animal Image Dataset - Cats, Dogs, and Foxes](https://www.kaggle.com/datasets/snmahsa/animal-image-dataset-cats-dogs-and-foxes)** - Дополнительный датасет для оценки эффективности решения на относительно простых задачах

  *Особенности:*
  - Среднее количество изображений на класс: **103**
  - Размеченные данные для сегментации: **отсутствуют**
  - Расположение объектов: **большинство расположено изолированно** друг от друга без перекрытий



## Основные возможности
* **Гибридный пайплайн:** YOLO12s генерирует bounding box'ы и начальные маски сегментации, SAM3 выполняет их уточнение для повышения качества.
* **Высокие метрики:** Достигнут cgF1 до **37.2** по бенчмаркам Sa-Co
* **Zero-shot & Open-Vocabulary:** Поддержка детекции и сегментации для классов, не встречавшихся при обучении
* **Ключевые метрики YOLO12s:**
    * `mAP50` = **0.489**
    * `mAP50-95` = **0.381**


## Принцип работы решения

Архитектура проекта реализует последовательный двухэтапный пайплайн, объединяющий скоростную детекцию и прецизионную сегментацию

### Этап 1: Детекция объектов (YOLO12s)
Для каждого целевого датасета идет предварительное обучение, после чео модель:
* **Обнаруживает** все объекты целевых классов на изображении.
* **Возвращает** для каждого объекта:
    * Координаты ограничивающего прямоугольника (**Bounding Box**)
    * Имя/класс объекта (**Label**)

### Этап 2: Точная сегментация (SAM3)
Затем полученные данные передаются в модель **SAM3**, которая выполняет уточняющую сегментацию. SAM3 поддерживает несколько режимов работы:

| Режим ввода | Описание | Применение |
|-------------|----------|------------|
| **По Bounding Box** | SAM3 получает координаты bbox и выполняет сегментацию строго в его границах. | Когда важна точная пространственная локализация. |
| **По текстовому описанию** | Передаются имена классов (например, "bottle-oil", "cat"), SAM3 ищет соответствующие объекты на всём изображении. | Для open-vocabulary сегментации, когда bbox недоступны. |
| **Комбинированный** | Одновременно передаются и bbox, и текстовые промпты. SAM3 использует оба сигнала для максимальной точности. | Основной режим работы пайплайна — использует сильные стороны обоих подходов. |


